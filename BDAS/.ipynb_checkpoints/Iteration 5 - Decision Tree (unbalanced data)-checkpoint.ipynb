{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section must be included at the beginning of each new notebook. Remember to change the app name. \n",
    "# If you're using VirtualBox, change the below to '/home/user/spark-2.1.1-bin-hadoop2.7'\n",
    "import findspark\n",
    "findspark.init('/opt/spark')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('basics').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read in the data. If you open the dataset, you'll find that each column has a header. We specify that by stating that header=True.\n",
    "# To make our lives easier, we can also use 'inferSchema' when importing CSVs. This automatically detects data types.\n",
    "# If you would like to manually change data types, refer to this article: https://medium.com/@mrpowers/adding-structtype-columns-to-spark-dataframes-b44125409803\n",
    "df = spark.read.csv('Datasets/aggreated_data.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data points2: 285331\n"
     ]
    }
   ],
   "source": [
    "df = df.na.drop()\n",
    "\n",
    "# df = df.na.drop(subset=\"road_surface_conditions\")\n",
    "# df = df.na.drop(subset=\"light_conditions\")\n",
    "# df = df.na.drop(subset=\"weather_conditions\")\n",
    "# df = df.na.drop(subset=\"accident_severity\")\n",
    "# df = df.na.drop(subset=\"day_of_week\")\n",
    "\n",
    "# df = df.na.drop(subset=\"special_conditions_at_site\")\n",
    "# df = df.na.drop(subset=\"pedestrian_movement\")\n",
    "df = df.na.drop(subset=\"age_of_vehicle\")\n",
    "df = df.na.drop(subset=\"sex_of_driver\")\n",
    "df = df.na.drop(subset=\"age_of_driver\")\n",
    "df = df.na.drop(subset=\"junction_location\")\n",
    "df = df.na.drop(subset=\"junction_detail\")\n",
    "df = df.na.drop(subset=\"junction_control\")\n",
    "df = df.na.drop(subset=\"day_of_week\")\n",
    "df = df.na.drop(subset=\"accident_severity\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Total data points2:\", df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "          'age_of_vehicle','sex_of_driver','age_of_driver','junction_location','junction_detail','junction_control','day_of_week','accident_severity']\n",
    "# features_s = ['light_conditions','weather_conditions']\n",
    "\n",
    "\n",
    "df1 = df.select(*features)\n",
    "df1 = df1.filter(df1.accident_severity > 1)\n",
    "df1 = df1.filter(df1.age_of_vehicle > 0)\n",
    "df1 = df1.filter(df1.junction_control > 0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "# categoricalColumns = ['road_surface_conditions','light_conditions','weather_conditions']\n",
    "\n",
    "ageofvehicle_indexer = StringIndexer(inputCol='age_of_vehicle',outputCol='ageofvehicleIndexer')\n",
    "sexofdriver_indexer = StringIndexer(inputCol='sex_of_driver',outputCol='sexofdriverIndexer')\n",
    "ageofdriver_indexer = StringIndexer(inputCol='age_of_driver',outputCol='ageofdriverIndexer')\n",
    "junctionlocation_indexer = StringIndexer(inputCol='junction_location',outputCol='junctionlocationIndexer')\n",
    "junctiondetail_indexer = StringIndexer(inputCol='junction_detail',outputCol='junctiondetailIndexer')\n",
    "junctioncontrol_indexer = StringIndexer(inputCol='junction_control',outputCol='junctioncontrolIndexer')\n",
    "dayofweek_indexer = StringIndexer(inputCol='day_of_week',outputCol='dayofweekIndexer')\n",
    "\n",
    "\n",
    "accidentSeverity_indexer = StringIndexer(inputCol='accident_severity',outputCol='label')\n",
    "\n",
    "\n",
    "# road_surface_conditions_encoder = OneHotEncoder(inputCol='road_surface_conditions_indexer',outputCol='road_surface_conditionsVec')\n",
    "ageofvehicle_encoder = OneHotEncoder(inputCol='ageofvehicleIndexer',outputCol='ageofvehicleVec')\n",
    "sexofdriver_encoder = OneHotEncoder(inputCol='sexofdriverIndexer',outputCol='sexofdriverVec')\n",
    "ageofdriver_encoder = OneHotEncoder(inputCol='ageofdriverIndexer',outputCol='ageofdriverVec')\n",
    "junctionlocation_encoder = OneHotEncoder(inputCol='junctionlocationIndexer',outputCol='junctionlocationVec')\n",
    "junctiondetail_encoder = OneHotEncoder(inputCol='junctiondetailIndexer',outputCol='junctiondetailVec')\n",
    "junctioncontrol_encoder = OneHotEncoder(inputCol='junctioncontrolIndexer',outputCol='junctioncontrolVec')\n",
    "dayofweek_encoder = OneHotEncoder(inputCol='dayofweekIndexer',outputCol='dayofweekVec')\n",
    "\n",
    "\n",
    "# And finally, using vector assembler to turn all of these columns into one column (named features).\n",
    "assembler = VectorAssembler(inputCols=['ageofvehicleVec','sexofdriverVec','ageofdriverVec','junctionlocationVec','junctiondetailVec','junctioncontrolVec','dayofweekVec'], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Then go through our steps. It's essentially sequential to the above.\n",
    "pipeline = Pipeline(stages=[ageofvehicle_indexer, sexofdriver_indexer, ageofdriver_indexer,junctionlocation_indexer,junctiondetail_indexer,junctioncontrol_indexer,dayofweek_indexer,\n",
    "                            accidentSeverity_indexer,\n",
    "                            ageofvehicle_encoder, sexofdriver_encoder,ageofdriver_encoder,junctionlocation_encoder,junctiondetail_encoder,junctioncontrol_encoder,dayofweek_encoder,\n",
    "                            assembler])\n",
    "\n",
    "# Now that we've got a number of steps, let's apply it to the DataFrame.\n",
    "pipeline_model = pipeline.fit(df1)\n",
    "\n",
    "# Incorporate results into a new DataFrame.\n",
    "pipe_df = pipeline_model.transform(df1)\n",
    "\n",
    "# Remove all variables other than features and label. \n",
    "pipe_df = pipe_df.select('label', 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 94471\n",
      "Test Dataset Count: 23842\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Split our data. Note that the new DataFrame is being used.\n",
    "train_data, test_data = pipe_df.randomSplit([0.8,0.2])\n",
    "print(\"Training Dataset Count: \" + str(train_data.count()))\n",
    "print(\"Test Dataset Count: \" + str(test_data.count()))\n",
    "\n",
    "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 3)\n",
    "dtModel = dt.fit(train_data)\n",
    "predictions = dtModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------+--------------------+----------+\n",
      "|label|            features| rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------+--------------------+----------+\n",
      "|  0.0|(175,[0,64,67,149...|[2454.0,176.0]|[0.93307984790874...|       0.0|\n",
      "|  0.0|(175,[0,64,67,149...|[2454.0,176.0]|[0.93307984790874...|       0.0|\n",
      "|  0.0|(175,[0,64,67,149...|[2454.0,176.0]|[0.93307984790874...|       0.0|\n",
      "|  0.0|(175,[0,64,67,149...|[2454.0,176.0]|[0.93307984790874...|       0.0|\n",
      "|  0.0|(175,[0,64,67,149...|[2454.0,176.0]|[0.93307984790874...|       0.0|\n",
      "|  0.0|(175,[0,64,67,149...|[2454.0,176.0]|[0.93307984790874...|       0.0|\n",
      "|  0.0|(175,[0,64,67,149...|[2454.0,176.0]|[0.93307984790874...|       0.0|\n",
      "|  0.0|(175,[0,64,67,149...|[2454.0,176.0]|[0.93307984790874...|       0.0|\n",
      "|  0.0|(175,[0,64,67,149...|  [741.0,22.0]|[0.97116644823066...|       0.0|\n",
      "|  0.0|(175,[0,64,67,149...|[2454.0,176.0]|[0.93307984790874...|       0.0|\n",
      "|  0.0|(175,[0,64,67,149...|  [741.0,22.0]|[0.97116644823066...|       0.0|\n",
      "|  0.0|(175,[0,64,67,149...|  [741.0,22.0]|[0.97116644823066...|       0.0|\n",
      "|  0.0|(175,[0,64,67,149...|  [741.0,22.0]|[0.97116644823066...|       0.0|\n",
      "|  0.0|(175,[0,64,67,149...|  [741.0,22.0]|[0.97116644823066...|       0.0|\n",
      "|  0.0|(175,[0,64,67,149...|[2454.0,176.0]|[0.93307984790874...|       0.0|\n",
      "|  0.0|(175,[0,64,67,149...|[2454.0,176.0]|[0.93307984790874...|       0.0|\n",
      "|  0.0|(175,[0,64,67,149...|[2454.0,176.0]|[0.93307984790874...|       0.0|\n",
      "|  0.0|(175,[0,64,67,149...|  [741.0,22.0]|[0.97116644823066...|       0.0|\n",
      "|  0.0|(175,[0,64,67,149...|  [741.0,22.0]|[0.97116644823066...|       0.0|\n",
      "|  0.0|(175,[0,64,67,150...|[2454.0,176.0]|[0.93307984790874...|       0.0|\n",
      "+-----+--------------------+--------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC: 0.5680705752862528\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
