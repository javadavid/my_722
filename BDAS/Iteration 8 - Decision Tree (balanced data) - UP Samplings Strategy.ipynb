{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section must be included at the beginning of each new notebook. Remember to change the app name. \n",
    "# If you're using VirtualBox, change the below to '/home/user/spark-2.1.1-bin-hadoop2.7'\n",
    "import findspark\n",
    "findspark.init('/opt/spark')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('basics').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read in the data. If you open the dataset, you'll find that each column has a header. We specify that by stating that header=True.\n",
    "# To make our lives easier, we can also use 'inferSchema' when importing CSVs. This automatically detects data types.\n",
    "# If you would like to manually change data types, refer to this article: https://medium.com/@mrpowers/adding-structtype-columns-to-spark-dataframes-b44125409803\n",
    "df = spark.read.csv('Datasets/aggreated_data.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data points2: 285331\n"
     ]
    }
   ],
   "source": [
    "df = df.na.drop()\n",
    "\n",
    "# df = df.na.drop(subset=\"road_surface_conditions\")\n",
    "# df = df.na.drop(subset=\"light_conditions\")\n",
    "# df = df.na.drop(subset=\"weather_conditions\")\n",
    "# df = df.na.drop(subset=\"accident_severity\")\n",
    "# df = df.na.drop(subset=\"day_of_week\")\n",
    "\n",
    "# df = df.na.drop(subset=\"special_conditions_at_site\")\n",
    "# df = df.na.drop(subset=\"pedestrian_movement\")\n",
    "df = df.na.drop(subset=\"age_of_vehicle\")\n",
    "df = df.na.drop(subset=\"sex_of_driver\")\n",
    "df = df.na.drop(subset=\"age_of_driver\")\n",
    "df = df.na.drop(subset=\"junction_location\")\n",
    "df = df.na.drop(subset=\"junction_detail\")\n",
    "df = df.na.drop(subset=\"junction_control\")\n",
    "df = df.na.drop(subset=\"day_of_week\")\n",
    "df = df.na.drop(subset=\"accident_severity\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Total data points2:\", df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "          'age_of_vehicle','sex_of_driver','age_of_driver','junction_location','junction_detail','junction_control','day_of_week','accident_severity']\n",
    "# features_s = ['light_conditions','weather_conditions']\n",
    "\n",
    "\n",
    "df1 = df.select(*features)\n",
    "df1 = df1.filter(df1.accident_severity > 1)\n",
    "df1 = df1.filter(df1.age_of_vehicle > 0)\n",
    "df1 = df1.filter(df1.junction_control > 0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87064"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.random import randint\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    " \n",
    "RATIO_ADJUST = 2.0 ## ratio of pos to neg in the df_subsample\n",
    " \n",
    "counts = df1.select('accident_severity').groupBy('accident_severity').count().collect()\n",
    "\n",
    "\n",
    "higherBound = counts[0][1]\n",
    "\n",
    "TRESHOLD_TO_FILTER = int(RATIO_ADJUST * float(counts[1][1]) / counts[0][1] * higherBound)\n",
    "\n",
    "\n",
    "randGen = lambda x: randint(0, higherBound) if x != '3' else -1\n",
    " \n",
    "udfRandGen = udf(randGen, IntegerType())\n",
    "df1 = df1.withColumn(\"randIndex\", udfRandGen(\"accident_severity\"))\n",
    "\n",
    "\n",
    "\n",
    "df1 = df1.filter(df1['randIndex'] > TRESHOLD_TO_FILTER)\n",
    "\n",
    "df1.count()\n",
    "#print(\"Before down-sample data ammount\", df1.count())\n",
    "#print(\"After down-sample data ammount\", df_subsample.count())\n",
    "#print(\"Distribution of 3 and 2 cases of the down-sampled training data are: \\n\", df_subsample.groupBy(\"accident_severity\").count().take(3))\n",
    "#df1 = df_subsample.drop('randIndex')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "# categoricalColumns = ['road_surface_conditions','light_conditions','weather_conditions']\n",
    "\n",
    "ageofvehicle_indexer = StringIndexer(inputCol='age_of_vehicle',outputCol='ageofvehicleIndexer').setHandleInvalid(\"skip\")\n",
    "sexofdriver_indexer = StringIndexer(inputCol='sex_of_driver',outputCol='sexofdriverIndexer').setHandleInvalid(\"skip\")\n",
    "ageofdriver_indexer = StringIndexer(inputCol='age_of_driver',outputCol='ageofdriverIndexer').setHandleInvalid(\"skip\")\n",
    "junctionlocation_indexer = StringIndexer(inputCol='junction_location',outputCol='junctionlocationIndexer').setHandleInvalid(\"skip\")\n",
    "junctiondetail_indexer = StringIndexer(inputCol='junction_detail',outputCol='junctiondetailIndexer').setHandleInvalid(\"skip\")\n",
    "junctioncontrol_indexer = StringIndexer(inputCol='junction_control',outputCol='junctioncontrolIndexer').setHandleInvalid(\"skip\")\n",
    "dayofweek_indexer = StringIndexer(inputCol='day_of_week',outputCol='dayofweekIndexer').setHandleInvalid(\"skip\")\n",
    "\n",
    "\n",
    "accidentSeverity_indexer = StringIndexer(inputCol='accident_severity',outputCol='label').setHandleInvalid(\"skip\")\n",
    "\n",
    "\n",
    "# road_surface_conditions_encoder = OneHotEncoder(inputCol='road_surface_conditions_indexer',outputCol='road_surface_conditionsVec')\n",
    "ageofvehicle_encoder = OneHotEncoder(inputCol='ageofvehicleIndexer',outputCol='ageofvehicleVec')\n",
    "sexofdriver_encoder = OneHotEncoder(inputCol='sexofdriverIndexer',outputCol='sexofdriverVec')\n",
    "ageofdriver_encoder = OneHotEncoder(inputCol='ageofdriverIndexer',outputCol='ageofdriverVec')\n",
    "junctionlocation_encoder = OneHotEncoder(inputCol='junctionlocationIndexer',outputCol='junctionlocationVec')\n",
    "junctiondetail_encoder = OneHotEncoder(inputCol='junctiondetailIndexer',outputCol='junctiondetailVec')\n",
    "junctioncontrol_encoder = OneHotEncoder(inputCol='junctioncontrolIndexer',outputCol='junctioncontrolVec')\n",
    "dayofweek_encoder = OneHotEncoder(inputCol='dayofweekIndexer',outputCol='dayofweekVec')\n",
    "\n",
    "\n",
    "# And finally, using vector assembler to turn all of these columns into one column (named features).\n",
    "assembler = VectorAssembler(inputCols=['ageofvehicleVec','sexofdriverVec','ageofdriverVec','junctionlocationVec','junctiondetailVec','junctioncontrolVec','dayofweekVec'], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Then go through our steps. It's essentially sequential to the above.\n",
    "pipeline = Pipeline(stages=[ageofvehicle_indexer, sexofdriver_indexer, ageofdriver_indexer,junctionlocation_indexer,junctiondetail_indexer,junctioncontrol_indexer,dayofweek_indexer,\n",
    "                            accidentSeverity_indexer,\n",
    "                            ageofvehicle_encoder, sexofdriver_encoder,ageofdriver_encoder,junctionlocation_encoder,junctiondetail_encoder,junctioncontrol_encoder,dayofweek_encoder,\n",
    "                            assembler])\n",
    "\n",
    "# Now that we've got a number of steps, let's apply it to the DataFrame.\n",
    "pipeline_model = pipeline.fit(df1)\n",
    "\n",
    "# Incorporate results into a new DataFrame.\n",
    "pipe_df = pipeline_model.transform(df1)\n",
    "\n",
    "# Remove all variables other than features and label. \n",
    "pipe_df = pipe_df.select('label', 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(170,[0,61,65,144...|\n",
      "|  0.0|(170,[1,61,73,146...|\n",
      "|  0.0|(170,[4,60,83,146...|\n",
      "|  0.0|(170,[0,60,66,145...|\n",
      "|  0.0|(170,[0,61,84,144...|\n",
      "|  0.0|(170,[0,61,68,144...|\n",
      "|  0.0|(170,[15,61,97,14...|\n",
      "|  0.0|(170,[8,60,80,145...|\n",
      "|  0.0|(170,[10,60,78,14...|\n",
      "|  0.0|(170,[1,60,142,14...|\n",
      "|  0.0|(170,[6,60,73,144...|\n",
      "|  0.0|(170,[0,61,97,144...|\n",
      "|  1.0|(170,[4,61,78,144...|\n",
      "|  0.0|(170,[1,61,114,14...|\n",
      "|  0.0|(170,[6,61,73,147...|\n",
      "|  0.0|(170,[4,60,78,146...|\n",
      "|  0.0|(170,[10,61,78,14...|\n",
      "|  0.0|(170,[9,61,87,146...|\n",
      "|  0.0|(170,[11,60,75,14...|\n",
      "|  0.0|(170,[0,61,75,148...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipe_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Split our data. Note that the new DataFrame is being used.\n",
    "train_data1, test_data1 = pipe_df.randomSplit([0.8,0.2])\n",
    "#print(\"Training Dataset Count: \" + str(train_data1.count()))\n",
    "#print(\"Test Dataset Count: \" + str(test_data1.count()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 3)\n",
    "dtModel = dt.fit(train_data1)\n",
    "\n",
    "predictions = dtModel.transform(test_data1)\n",
    "\n",
    "#setHandleInvalid(\"skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------+--------------------+----------+\n",
      "|label|            features| rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------+--------------------+----------+\n",
      "|  0.0|(170,[0,60,63,144...|[1798.0,130.0]|[0.93257261410788...|       0.0|\n",
      "|  0.0|(170,[0,60,63,144...|[1798.0,130.0]|[0.93257261410788...|       0.0|\n",
      "|  0.0|(170,[0,60,63,144...|[1798.0,130.0]|[0.93257261410788...|       0.0|\n",
      "|  0.0|(170,[0,60,63,144...|[1798.0,130.0]|[0.93257261410788...|       0.0|\n",
      "|  0.0|(170,[0,60,63,144...|[1798.0,130.0]|[0.93257261410788...|       0.0|\n",
      "|  0.0|(170,[0,60,63,144...|[1798.0,130.0]|[0.93257261410788...|       0.0|\n",
      "|  0.0|(170,[0,60,63,144...|[1798.0,130.0]|[0.93257261410788...|       0.0|\n",
      "|  0.0|(170,[0,60,63,144...|[1798.0,130.0]|[0.93257261410788...|       0.0|\n",
      "|  0.0|(170,[0,60,63,144...|[1798.0,130.0]|[0.93257261410788...|       0.0|\n",
      "|  0.0|(170,[0,60,63,144...|[1798.0,130.0]|[0.93257261410788...|       0.0|\n",
      "|  0.0|(170,[0,60,63,144...|[8071.0,832.0]|[0.90654835448725...|       0.0|\n",
      "|  0.0|(170,[0,60,63,144...|[1798.0,130.0]|[0.93257261410788...|       0.0|\n",
      "|  0.0|(170,[0,60,63,144...|[8071.0,832.0]|[0.90654835448725...|       0.0|\n",
      "|  0.0|(170,[0,60,63,144...|[8071.0,832.0]|[0.90654835448725...|       0.0|\n",
      "|  0.0|(170,[0,60,63,144...|[1798.0,130.0]|[0.93257261410788...|       0.0|\n",
      "|  0.0|(170,[0,60,63,144...|[1798.0,130.0]|[0.93257261410788...|       0.0|\n",
      "|  0.0|(170,[0,60,63,145...|[1798.0,130.0]|[0.93257261410788...|       0.0|\n",
      "|  0.0|(170,[0,60,63,145...|[1798.0,130.0]|[0.93257261410788...|       0.0|\n",
      "|  0.0|(170,[0,60,63,145...|[1798.0,130.0]|[0.93257261410788...|       0.0|\n",
      "|  0.0|(170,[0,60,63,145...|[1798.0,130.0]|[0.93257261410788...|       0.0|\n",
      "+-----+--------------------+--------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC: 0.5689420614444342\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
