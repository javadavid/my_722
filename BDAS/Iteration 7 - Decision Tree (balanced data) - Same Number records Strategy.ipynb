{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section must be included at the beginning of each new notebook. Remember to change the app name. \n",
    "# If you're using VirtualBox, change the below to '/home/user/spark-2.1.1-bin-hadoop2.7'\n",
    "import findspark\n",
    "findspark.init('/opt/spark')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('basics').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read in the data. If you open the dataset, you'll find that each column has a header. We specify that by stating that header=True.\n",
    "# To make our lives easier, we can also use 'inferSchema' when importing CSVs. This automatically detects data types.\n",
    "# If you would like to manually change data types, refer to this article: https://medium.com/@mrpowers/adding-structtype-columns-to-spark-dataframes-b44125409803\n",
    "df = spark.read.csv('Datasets/aggreated_data.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data points2: 285331\n"
     ]
    }
   ],
   "source": [
    "df = df.na.drop()\n",
    "\n",
    "# df = df.na.drop(subset=\"road_surface_conditions\")\n",
    "# df = df.na.drop(subset=\"light_conditions\")\n",
    "# df = df.na.drop(subset=\"weather_conditions\")\n",
    "# df = df.na.drop(subset=\"accident_severity\")\n",
    "# df = df.na.drop(subset=\"day_of_week\")\n",
    "\n",
    "# df = df.na.drop(subset=\"special_conditions_at_site\")\n",
    "# df = df.na.drop(subset=\"pedestrian_movement\")\n",
    "df = df.na.drop(subset=\"age_of_vehicle\")\n",
    "df = df.na.drop(subset=\"sex_of_driver\")\n",
    "df = df.na.drop(subset=\"age_of_driver\")\n",
    "df = df.na.drop(subset=\"junction_location\")\n",
    "df = df.na.drop(subset=\"junction_detail\")\n",
    "df = df.na.drop(subset=\"junction_control\")\n",
    "df = df.na.drop(subset=\"day_of_week\")\n",
    "df = df.na.drop(subset=\"accident_severity\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Total data points2:\", df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "          'age_of_vehicle','sex_of_driver','age_of_driver','junction_location','junction_detail','junction_control','day_of_week','accident_severity']\n",
    "# features_s = ['light_conditions','weather_conditions']\n",
    "\n",
    "\n",
    "df1 = df.select(*features)\n",
    "df1 = df1.filter(df1.accident_severity > 1)\n",
    "df1 = df1.filter(df1.age_of_vehicle > 0)\n",
    "df1 = df1.filter(df1.junction_control > 0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13862\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df2 = df1.filter(df1.accident_severity == 2)\n",
    "print(df2.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13862\n"
     ]
    }
   ],
   "source": [
    "df3 = df1.filter(df1.accident_severity == 3).limit(df2.count())\n",
    "print(df3.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27724\n"
     ]
    }
   ],
   "source": [
    "df1 = df2.union(df3)\n",
    "print(df1.count()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "# categoricalColumns = ['road_surface_conditions','light_conditions','weather_conditions']\n",
    "\n",
    "ageofvehicle_indexer = StringIndexer(inputCol='age_of_vehicle',outputCol='ageofvehicleIndexer').setHandleInvalid(\"skip\")\n",
    "sexofdriver_indexer = StringIndexer(inputCol='sex_of_driver',outputCol='sexofdriverIndexer').setHandleInvalid(\"skip\")\n",
    "ageofdriver_indexer = StringIndexer(inputCol='age_of_driver',outputCol='ageofdriverIndexer').setHandleInvalid(\"skip\")\n",
    "junctionlocation_indexer = StringIndexer(inputCol='junction_location',outputCol='junctionlocationIndexer').setHandleInvalid(\"skip\")\n",
    "junctiondetail_indexer = StringIndexer(inputCol='junction_detail',outputCol='junctiondetailIndexer').setHandleInvalid(\"skip\")\n",
    "junctioncontrol_indexer = StringIndexer(inputCol='junction_control',outputCol='junctioncontrolIndexer').setHandleInvalid(\"skip\")\n",
    "dayofweek_indexer = StringIndexer(inputCol='day_of_week',outputCol='dayofweekIndexer').setHandleInvalid(\"skip\")\n",
    "\n",
    "\n",
    "accidentSeverity_indexer = StringIndexer(inputCol='accident_severity',outputCol='label').setHandleInvalid(\"skip\")\n",
    "\n",
    "\n",
    "# road_surface_conditions_encoder = OneHotEncoder(inputCol='road_surface_conditions_indexer',outputCol='road_surface_conditionsVec')\n",
    "ageofvehicle_encoder = OneHotEncoder(inputCol='ageofvehicleIndexer',outputCol='ageofvehicleVec')\n",
    "sexofdriver_encoder = OneHotEncoder(inputCol='sexofdriverIndexer',outputCol='sexofdriverVec')\n",
    "ageofdriver_encoder = OneHotEncoder(inputCol='ageofdriverIndexer',outputCol='ageofdriverVec')\n",
    "junctionlocation_encoder = OneHotEncoder(inputCol='junctionlocationIndexer',outputCol='junctionlocationVec')\n",
    "junctiondetail_encoder = OneHotEncoder(inputCol='junctiondetailIndexer',outputCol='junctiondetailVec')\n",
    "junctioncontrol_encoder = OneHotEncoder(inputCol='junctioncontrolIndexer',outputCol='junctioncontrolVec')\n",
    "dayofweek_encoder = OneHotEncoder(inputCol='dayofweekIndexer',outputCol='dayofweekVec')\n",
    "\n",
    "\n",
    "# And finally, using vector assembler to turn all of these columns into one column (named features).\n",
    "assembler = VectorAssembler(inputCols=['ageofvehicleVec','sexofdriverVec','ageofdriverVec','junctionlocationVec','junctiondetailVec','junctioncontrolVec','dayofweekVec'], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Then go through our steps. It's essentially sequential to the above.\n",
    "pipeline = Pipeline(stages=[ageofvehicle_indexer, sexofdriver_indexer, ageofdriver_indexer,junctionlocation_indexer,junctiondetail_indexer,junctioncontrol_indexer,dayofweek_indexer,\n",
    "                            accidentSeverity_indexer,\n",
    "                            ageofvehicle_encoder, sexofdriver_encoder,ageofdriver_encoder,junctionlocation_encoder,junctiondetail_encoder,junctioncontrol_encoder,dayofweek_encoder,\n",
    "                            assembler])\n",
    "\n",
    "# Now that we've got a number of steps, let's apply it to the DataFrame.\n",
    "pipeline_model = pipeline.fit(df1)\n",
    "\n",
    "# Incorporate results into a new DataFrame.\n",
    "pipe_df = pipeline_model.transform(df1)\n",
    "\n",
    "# Remove all variables other than features and label. \n",
    "pipe_df = pipe_df.select('label', 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(164,[7,54,71,138...|\n",
      "|  0.0|(164,[2,53,57,143...|\n",
      "|  0.0|(164,[0,53,86,139...|\n",
      "|  0.0|(164,[8,53,87,138...|\n",
      "|  0.0|(164,[24,54,114,1...|\n",
      "|  0.0|(164,[4,53,57,138...|\n",
      "|  0.0|(164,[9,53,78,139...|\n",
      "|  0.0|(164,[1,54,64,139...|\n",
      "|  0.0|(164,[4,53,58,140...|\n",
      "|  0.0|(164,[6,53,78,140...|\n",
      "|  0.0|(164,[0,53,81,138...|\n",
      "|  0.0|(164,[0,53,81,138...|\n",
      "|  0.0|(164,[1,53,78,138...|\n",
      "|  0.0|(164,[0,53,66,139...|\n",
      "|  0.0|(164,[4,53,57,139...|\n",
      "|  0.0|(164,[2,53,91,138...|\n",
      "|  0.0|(164,[1,53,71,138...|\n",
      "|  0.0|(164,[6,53,60,138...|\n",
      "|  0.0|(164,[0,53,63,138...|\n",
      "|  0.0|(164,[7,53,100,13...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipe_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Split our data. Note that the new DataFrame is being used.\n",
    "train_data1, test_data1 = pipe_df.randomSplit([0.8,0.2])\n",
    "#print(\"Training Dataset Count: \" + str(train_data1.count()))\n",
    "#print(\"Test Dataset Count: \" + str(test_data1.count()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 3)\n",
    "dtModel = dt.fit(train_data1)\n",
    "\n",
    "predictions = dtModel.transform(test_data1)\n",
    "\n",
    "#setHandleInvalid(\"skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---------------+--------------------+----------+\n",
      "|label|            features|  rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+---------------+--------------------+----------+\n",
      "|  0.0|(164,[0,53,56,138...|  [136.0,512.0]|[0.20987654320987...|       1.0|\n",
      "|  0.0|(164,[0,53,56,138...|  [136.0,512.0]|[0.20987654320987...|       1.0|\n",
      "|  0.0|(164,[0,53,56,139...|   [50.0,383.0]|[0.11547344110854...|       1.0|\n",
      "|  0.0|(164,[0,53,56,140...|[5165.0,3237.0]|[0.61473458700309...|       0.0|\n",
      "|  0.0|(164,[0,53,57,140...|[5165.0,3237.0]|[0.61473458700309...|       0.0|\n",
      "|  0.0|(164,[0,53,58,138...|[3260.0,3946.0]|[0.45240077713016...|       1.0|\n",
      "|  0.0|(164,[0,53,58,140...|[5165.0,3237.0]|[0.61473458700309...|       0.0|\n",
      "|  0.0|(164,[0,53,59,144...|     [19.0,0.0]|           [1.0,0.0]|       0.0|\n",
      "|  0.0|(164,[0,53,60,138...|[3260.0,3946.0]|[0.45240077713016...|       1.0|\n",
      "|  0.0|(164,[0,53,60,138...|[3260.0,3946.0]|[0.45240077713016...|       1.0|\n",
      "|  0.0|(164,[0,53,60,138...|[3260.0,3946.0]|[0.45240077713016...|       1.0|\n",
      "|  0.0|(164,[0,53,60,139...|[5165.0,3237.0]|[0.61473458700309...|       0.0|\n",
      "|  0.0|(164,[0,53,61,138...|[3260.0,3946.0]|[0.45240077713016...|       1.0|\n",
      "|  0.0|(164,[0,53,61,139...|[1469.0,2899.0]|[0.33630952380952...|       1.0|\n",
      "|  0.0|(164,[0,53,61,141...|  [910.0,109.0]|[0.89303238469087...|       0.0|\n",
      "|  0.0|(164,[0,53,62,138...|[3260.0,3946.0]|[0.45240077713016...|       1.0|\n",
      "|  0.0|(164,[0,53,63,138...|[3260.0,3946.0]|[0.45240077713016...|       1.0|\n",
      "|  0.0|(164,[0,53,63,138...|[1469.0,2899.0]|[0.33630952380952...|       1.0|\n",
      "|  0.0|(164,[0,53,63,139...|[5165.0,3237.0]|[0.61473458700309...|       0.0|\n",
      "|  0.0|(164,[0,53,63,139...|[5165.0,3237.0]|[0.61473458700309...|       0.0|\n",
      "+-----+--------------------+---------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC: 0.49071218587008064\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
